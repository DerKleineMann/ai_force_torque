{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file regroups every kind of test for model training : \n",
    "- The first file & library imports are mandatory\n",
    "- The last exports are mandatory\n",
    "- The model training depends on the choice of algorithm\n",
    "\n",
    "There is an option to automate the training using togglable parameters \n",
    "Make sure to have set up the correct file structure using env_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# TensorFlow / Keras Part\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense\n",
    "from keras.preprocessing import timeseries_dataset_from_array\n",
    "import keras_tuner as kt\n",
    "\n",
    "# SKLearn Part\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic constant file parameters\n",
    "FILENAME=\"model_training.ipynb\"\n",
    "DIRNAME=os.path.abspath(FILENAME).replace(FILENAME,'')\n",
    "\n",
    "# Datafile parameters\n",
    "DATA_FOLDER = \"versuch_f1\"\n",
    "VERSION_NB = 1\n",
    "DF_POINTS_RANGE = 11\n",
    "DF_POINTS_LENGTH = 3000\n",
    "\n",
    "# Data parameters\n",
    "OFFSET_XYZ=[-578.6,261.4,-375]\n",
    "CARTESIAN_COLUMNS=['curCart_x','comdCart_x','curCart_y','comdCart_y','curCart_z','comdCart_z']\n",
    "\n",
    "# Model parameters\n",
    "MODEL_TYPE = \"ffnn_regression\"\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "VALID_TRAIN_RATIO = 0.2\n",
    "MAX_TRIALS = 5 \n",
    "MAX_EXECUTIONS = 3\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_import(path):\n",
    "    # Read JSON file\n",
    "    with open(path,\"r\") as file :\n",
    "        data = json.load(file)\n",
    "    for variable in data :\n",
    "        globals()[f'{variable}'] = data[variable]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import():\n",
    "    # Load DataFrame\n",
    "    full_df=pd.read_feather(DIRNAME+\"data/feather/\"+DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)+\".feather\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    full_df=full_df.drop([\"t_Sec\",\"t_nSec\",\"Fx1\",\"Fy1\",\"Fz1\",\"Tx1\",\"Ty1\",\"Tz1\"],axis=1)\n",
    "\n",
    "    # Transform position from absolute to relative\n",
    "    for i in range(len(CARTESIAN_COLUMNS)):\n",
    "        full_df[CARTESIAN_COLUMNS[i]] = full_df[CARTESIAN_COLUMNS[i]] + OFFSET_XYZ[i//2]\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classes(df):\n",
    "        for i in range(0,df.shape[0]):\n",
    "            # Recovering cartesian positions\n",
    "            x = df['curCart_x'][i]\n",
    "            y = df['curCart_y'][i]\n",
    "            # Calculating classes\n",
    "            f1 = y - 2*x\n",
    "            f2 = y + 2*x\n",
    "            f3 = y - x/2\n",
    "            f4 = y + x/2\n",
    "            # class 1\n",
    "            if (f1<0) & (f3>=0):\n",
    "                df.loc[i,'position']='class 1'\n",
    "            # class 2\n",
    "            elif (f3<0) & (f4>=0):\n",
    "                df.loc[i,'position']='class 2'\n",
    "            # class 3\n",
    "            elif (f4<0) & (f2>=0):\n",
    "                df.loc[i,'position']='class 3'\n",
    "            # class 4\n",
    "            elif (f2<0) & (f1<=0):\n",
    "                df.loc[i,'position']='class 4'\n",
    "            # class 5\n",
    "            elif (f1>0) & (f3<=0):\n",
    "                df.loc[i,'position']='class 5'\n",
    "            # class 6\n",
    "            elif (f3>0) & (f4<=0):\n",
    "                df.loc[i,'position']='class 6'\n",
    "            # class 7\n",
    "            elif (f4>0) & (f2<=0):\n",
    "                df.loc[i,'position']='class 7'\n",
    "            # class 8\n",
    "            elif (f2>0) & (f1>=0):\n",
    "                df.loc[i,'position']='class 8'\n",
    "        return df\n",
    "\n",
    "def time_series_transform(df):\n",
    "    timeseries_df = timeseries_dataset_from_array(df, None, sequence_length=10,shuffle=False)\n",
    "    el = []\n",
    "    for element in timeseries_df:\n",
    "        el.append(element.numpy())\n",
    "    el_full = np.concatenate(el, axis=0)\n",
    "    return el_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    shuffled_df=df.sample(frac=1,random_state=1)\n",
    "\n",
    "    # X & Y datasets\n",
    "\n",
    "    if MODEL_TYPE == \"ffnn_regression\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Regression Y\n",
    "        Y = shuffled_df[['curCart_x','curCart_y','curCart_z']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"ffnn_classification\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Classification Y\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        Y = shuffled_df[['position']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_regression\" :\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['curCart_x','curCart_y']][9:].to_numpy())\n",
    "        Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_classification\" :\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key','position']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE11):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['position']][9:].to_numpy())\n",
    "        Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    else :\n",
    "        raise ValueError(\"MODEL_TYPE parameter is not valid\")\n",
    "\n",
    "\n",
    "    # Train / Test / Validation dataset\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=TEST_TRAIN_RATIO, random_state=2)\n",
    "    X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_Train, Y_Train, test_size=VALID_TRAIN_RATIO, random_state=3)\n",
    "\n",
    "    return X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - FFNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):    # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class FFNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):    # Main\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Tune number of layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                    )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):        # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class CNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layers\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"mse\", \n",
    "                    metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - FFNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                     )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                      loss=\"categorical_crossentropy\", \n",
    "                      metrics=[\"accuracy\",\"precision\",\"recall\",\"f1_score\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layer\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"categorical_crossentropy\", \n",
    "                    metrics=[\"accuracy\",\"precision\",\"recall\",\"f1_score\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 - RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_RandomSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.RandomSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=MAX_EXECUTIONS,\n",
    "        overwrite=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_GridSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.GridSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3 - Bayesian Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_BayesianSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        alpha=0.0001,\n",
    "        beta=2.6,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test1_0.json', 'test1_1.json', 'test1_2.json', 'test1_3.json']\n",
      ">> File Imported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset Split\n",
      ">> Model Loaded\n",
      ">> Tuner Searching\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_layers\n",
      "352               |352               |units_0\n",
      "tanh              |tanh              |activation\n",
      "False             |False             |dropout\n",
      "0.001423          |0.001423          |lr\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/quentin/Documents/ai_force_torque/model_training.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m>> Tuner Searching\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Search\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(X_Train, Y_Train, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_Valid, Y_Valid),callbacks\u001b[39m=\u001b[39;49m[keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(\u001b[39m'\u001b[39;49m\u001b[39mresults/tensorboard/\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Export results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m best_hps\u001b[39m=\u001b[39mtuner\u001b[39m.\u001b[39mget_best_hyperparameters()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:233\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 233\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:273\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m    272\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    274\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[1;32m    275\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:238\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 238\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[1;32m    240\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[1;32m    241\u001b[0m     ):\n\u001b[1;32m    242\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    246\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    254\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[1;32m    316\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[39m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmulti_backend():\n",
      "\u001b[1;32m/home/quentin/Documents/ai_force_torque/model_training.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X50sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs,batch_size\u001b[39m=\u001b[39;49mhp\u001b[39m.\u001b[39;49mChoice(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39m16\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m64\u001b[39;49m]),\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1808\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    906\u001b[0m         args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    907\u001b[0m     )\n\u001b[1;32m    908\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m   bound_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\n\u001b[1;32m    910\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds\n\u001b[1;32m    911\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1492\u001b[0m   )\n\u001b[1;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# JSON directory scan\n",
    "CONFIG_DIR=DIRNAME+\"config\"\n",
    "JSON_files = [file for file in os.listdir(CONFIG_DIR) if file.endswith(\".json\")]\n",
    "JSON_files.sort()\n",
    "\n",
    "print(JSON_files)\n",
    "\n",
    "for JSON_file in JSON_files :\n",
    "    # Import\n",
    "    json_import(CONFIG_DIR+\"/\"+JSON_file)\n",
    "    print(\">> File Imported\")\n",
    "    # Prepare\n",
    "    data_df = data_import()\n",
    "    # Split\n",
    "    X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid=data_split(data_df)\n",
    "    print(\">> Dataset Split\")\n",
    "\n",
    "    # Model selector\n",
    "    if (MODEL_TYPE == \"ffnn_regression\") :\n",
    "        model = FFNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"ffnn_classification\") :\n",
    "        model = FFNN_Classification_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_regression\") :\n",
    "        model = CNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_classification\") :\n",
    "        model = CNN_Classification_Model()\n",
    "\n",
    "    print(\">> Model Loaded\")\n",
    "\n",
    "    tuners_list = [KT_RandomSearch(model),KT_GridSearch(model),KT_BayesianSearch(model)]\n",
    "\n",
    "    for i in range(3) :\n",
    "        tuner = tuners_list[i]\n",
    "        print(\">> Tuner Searching\")\n",
    "        # Search\n",
    "        tuner.search(X_Train, Y_Train, epochs=15, validation_data=(X_Valid, Y_Valid),callbacks=[keras.callbacks.TensorBoard('results/tensorboard/')])\n",
    "        # Export results\n",
    "        best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "        # Save Results\n",
    "        with open(DIRNAME+\"results/models/\"+DATA_FOLDER+\"_\"+MODEL_TYPE+\"_\"+str(VERSION_NB)+\"_\"+str(i+1)+\".obj\",\"w\") as result_file :\n",
    "            json.dump(best_hps.values,result_file)\n",
    "\n",
    "    print(\"Program Successful !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
