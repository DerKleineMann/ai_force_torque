{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file regroups every kind of test for model training : \n",
    "- The first file & library imports are mandatory\n",
    "- The last exports are mandatory\n",
    "- The model training depends on the choice of algorithm\n",
    "\n",
    "There is an option to automate the training using togglable parameters \n",
    "Make sure to have set up the correct file structure using env_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ML Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set import parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_JSON_SUPPORTED = False\n",
    "DATA_FOLDER = \"versuch_f1\"\n",
    "VERSION_NB = 1\n",
    "FILENAME=\"model_training.ipynb\"\n",
    "DIRNAME=os.path.abspath(FILENAME).replace(FILENAME,'')\n",
    "DF_POINTS_RANGE = 11\n",
    "DF_POINTS_LENGTH = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "full_df=pd.read_feather(DIRNAME+\"data/feather/\"+DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)+\".feather\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "full_df=full_df.drop([\"t_Sec\",\"t_nSec\",\"Fx1\",\"Fy1\",\"Fz1\",\"Tx1\",\"Ty1\",\"Tz1\"],axis=1)\n",
    "\n",
    "# Transform position from absolute to relative\n",
    "OFFSET_XYZ=[-578.6,261.4,-375]\n",
    "CARTESIAN_COLUMNS=['curCart_x','comdCart_x','curCart_y','comdCart_y','curCart_z','comdCart_z']\n",
    "for i in range(len(CARTESIAN_COLUMNS)):\n",
    "    full_df[CARTESIAN_COLUMNS[i]] = full_df[CARTESIAN_COLUMNS[i]] + OFFSET_XYZ[i//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "TEST_TRAIN_RATIO = 0.2\n",
    "VALID_TRAIN_RATIO = 0.2\n",
    "\n",
    "# Shuffle DataFrame\n",
    "shuffled_df=full_df.sample(frac=1,random_state=1)\n",
    "\n",
    "# X & Y datasets\n",
    "X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "Y = shuffled_df[['curCart_x','curCart_y','curCart_z']].to_numpy()\n",
    "\n",
    "# Train / Test / Validation dataset\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=TEST_TRAIN_RATIO, random_state=2)\n",
    "X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_Train, Y_Train, test_size=VALID_TRAIN_RATIO, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_reg(hidden_layer_dim):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input((11,11,3000,20)))\n",
    "    # Hidden layers\n",
    "    # Output layer\n",
    "    model.add(Dense(3,activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - FFNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - FFNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 - RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 - GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3 - Chained GridSearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
