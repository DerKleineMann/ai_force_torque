{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file regroups every kind of test for model training : \n",
    "- The first file & library imports are mandatory\n",
    "- The last exports are mandatory\n",
    "- The model training depends on the choice of algorithm\n",
    "\n",
    "There is an option to automate the training using togglable parameters \n",
    "Make sure to have set up the correct file structure using env_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# TensorFlow / Keras Part\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense\n",
    "import keras_tuner as kt\n",
    "\n",
    "# SKLearn Part\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic constant file parameters\n",
    "FILENAME=\"model_training.ipynb\"\n",
    "DIRNAME=os.path.abspath(FILENAME).replace(FILENAME,'')\n",
    "\n",
    "# JSON Automatic Handling Switch\n",
    "IS_JSON_SUPPORTED = False\n",
    "\n",
    "# Datafile parameters\n",
    "DATA_FOLDER = \"versuch_f1\"\n",
    "VERSION_NB = 1\n",
    "DF_POINTS_RANGE = 11\n",
    "DF_POINTS_LENGTH = 3000\n",
    "\n",
    "# Data parameters\n",
    "OFFSET_XYZ=[-578.6,261.4,-375]\n",
    "CARTESIAN_COLUMNS=['curCart_x','comdCart_x','curCart_y','comdCart_y','curCart_z','comdCart_z']\n",
    "\n",
    "# Model parameters\n",
    "MODEL_TYPE = \"Regression\"\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "VALID_TRAIN_RATIO = 0.2\n",
    "MAX_TRIALS = 5 \n",
    "MAX_EXECUTIONS = 3\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSON_import():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import():\n",
    "    # Load DataFrame\n",
    "    full_df=pd.read_feather(DIRNAME+\"data/feather/\"+DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)+\".feather\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    full_df=full_df.drop([\"t_Sec\",\"t_nSec\",\"Fx1\",\"Fy1\",\"Fz1\",\"Tx1\",\"Ty1\",\"Tz1\"],axis=1)\n",
    "\n",
    "    # Transform position from absolute to relative\n",
    "    for i in range(len(CARTESIAN_COLUMNS)):\n",
    "        full_df[CARTESIAN_COLUMNS[i]] = full_df[CARTESIAN_COLUMNS[i]] + OFFSET_XYZ[i//2]\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classes(df):\n",
    "        for i in range(0,df.shape[0]):\n",
    "            # Recovering cartesian positions\n",
    "            x = df['curCart_x'][i]\n",
    "            y = df['curCart_y'][i]\n",
    "            # Calculating classes\n",
    "            f1 = y - 2*x\n",
    "            f2 = y + 2*x\n",
    "            f3 = y - x/2\n",
    "            f4 = y + x/2\n",
    "            # class 1\n",
    "            if (f1<0) & (f3>=0):\n",
    "                df.loc[i,'position']='class 1'\n",
    "            # class 2\n",
    "            elif (f3<0) & (f4>=0):\n",
    "                df.loc[i,'position']='class 2'\n",
    "            # class 3\n",
    "            elif (f4<0) & (f2>=0):\n",
    "                df.loc[i,'position']='class 3'\n",
    "            # class 4\n",
    "            elif (f2<0) & (f1<=0):\n",
    "                df.loc[i,'position']='class 4'\n",
    "            # class 5\n",
    "            elif (f1>0) & (f3<=0):\n",
    "                df.loc[i,'position']='class 5'\n",
    "            # class 6\n",
    "            elif (f3>0) & (f4<=0):\n",
    "                df.loc[i,'position']='class 6'\n",
    "            # class 7\n",
    "            elif (f4>0) & (f2<=0):\n",
    "                df.loc[i,'position']='class 7'\n",
    "            # class 8\n",
    "            elif (f2>0) & (f1>=0):\n",
    "                df.loc[i,'position']='class 8'\n",
    "        return df\n",
    "\n",
    "def time_series_transform(df):\n",
    "    timeseries_df = timeseries_dataset_from_array(df, None, sequence_length=10,shuffle=False)\n",
    "    el = []\n",
    "    for element in timeseries_df:\n",
    "        el.append(element.numpy())\n",
    "    el_full = np.concatenate(el, axis=0)\n",
    "    return el_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split():\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    shuffled_df=full_df.sample(frac=1,random_state=1)\n",
    "\n",
    "    # X & Y datasets\n",
    "\n",
    "    if MODEL_TYPE == \"ffnn_regression\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Regression Y\n",
    "        Y = shuffled_df[['curCart_x','curCart_y','curCart_z']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"ffnn_classification\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Classification Y\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        Y = shuffled_df[['position']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_regression\" :\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        ts_input = []\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                ts_input.append(locals()[f'ts_key{i}_{j}'])\n",
    "        ts_input = np.concatenate(ts_input, axis=0)\n",
    "        # Define Output\n",
    "        ts_output = []\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                ts_output.append(locals()[f'key{i}_{j}'][['curCart_x','curCart_y']][9:].to_numpy())\n",
    "        ts_output = np.concatenate(ts_output, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_classification\" :\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key','position']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        ts_input = []\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                ts_input.append(locals()[f'ts_key{i}_{j}'])\n",
    "        ts_input = np.concatenate(ts_input, axis=0)\n",
    "        ts_output = []\n",
    "        for i in range(0,11):\n",
    "            for j in range(0,11):\n",
    "                ts_output.append(locals()[f'key{i}_{j}'][['curCart_x','curCart_y']][9:].to_numpy())\n",
    "        ts_output = np.concatenate(ts_output, axis=0)\n",
    "\n",
    "    else :\n",
    "        raise ValueError(\"MODEL_TYPE parameter is not valid\")\n",
    "\n",
    "\n",
    "    # Train / Test / Validation dataset\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=TEST_TRAIN_RATIO, random_state=2)\n",
    "    X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_Train, Y_Train, test_size=VALID_TRAIN_RATIO, random_state=3)\n",
    "\n",
    "    return X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - FFNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def rmse(y_true,y_pred):    # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "    def rsquared(y_true,y_pred):    # Private\n",
    "        RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "        TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "        return 1-(RSS/TSS)\n",
    "\n",
    "    def build_model(hp):    # Main\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Tune number of layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                    )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def rmse(y_true,y_pred):        # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "    def rsquared(y_true,y_pred):    # Private\n",
    "        RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "        TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "        return 1-(RSS/TSS)\n",
    "\n",
    "    def build_model(hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layers\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"mse\", \n",
    "                    metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - FFNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build_model(hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layer\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"categorical_crossentropy\", \n",
    "                    metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 - RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_random_search(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.RandomSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=MAX_EXECUTIONS,\n",
    "        overwrite=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner\n",
    "\n",
    "    # tuner.search + TensorBoard\n",
    "    # # Export results\n",
    "    # best_model=tuner.get_best_models(num_models=2)[0]\n",
    "    # best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # return best_model,best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_GridSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    # Export results\n",
    "\n",
    "    return best_model,best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3 - Bayesian Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_BayesianSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    # Export results\n",
    "\n",
    "    return best_model,best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Save():\n",
    "    # Save model to file\n",
    "    # Save HPs to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selector():\n",
    "    if 0 :\n",
    "        FFNN_Regression_Model\n",
    "    elif 1 :\n",
    "        CNN_Regression_Model\n",
    "    elif 2 :\n",
    "        FFNN_Classification_Model\n",
    "    elif 3 :\n",
    "        CNN_Classification_Model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "\n",
    "# Import\n",
    "# Prepare\n",
    "# Split\n",
    "\n",
    "# Model selector\n",
    "# Model\n",
    "\n",
    "# HP\n",
    "\n",
    "# Search selector\n",
    "# Search\n",
    "\n",
    "# Epoch opti\n",
    "\n",
    "# Save Model\n",
    "# Save Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
