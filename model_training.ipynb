{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file regroups every kind of test for model training : \n",
    "- The first file & library imports are mandatory\n",
    "- The last exports are mandatory\n",
    "- The model training depends on the choice of algorithm\n",
    "\n",
    "There is an option to automate the training using togglable parameters \n",
    "Make sure to have set up the correct file structure using env_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 15:49:42.927370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-10 15:49:42.927453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-10 15:49:42.971664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-10 15:49:43.105780: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-10 15:49:44.122039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# TensorFlow / Keras Part\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Input, GlobalAveragePooling1D, Dropout\n",
    "from keras.preprocessing import timeseries_dataset_from_array\n",
    "import keras_tuner as kt\n",
    "\n",
    "# SKLearn Part\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic constant file parameters\n",
    "FILENAME=\"model_training.ipynb\"\n",
    "DIRNAME=os.path.abspath(FILENAME).replace(FILENAME,'')\n",
    "\n",
    "# Datafile parameters\n",
    "DATA_FOLDER = \"versuch_f1\"\n",
    "VERSION_NB = 1\n",
    "DF_POINTS_RANGE = 11\n",
    "DF_POINTS_LENGTH = 3000\n",
    "\n",
    "# Data parameters\n",
    "OFFSET_XYZ=[-578.6,261.4,-375]\n",
    "CARTESIAN_COLUMNS=['curCart_x','comdCart_x','curCart_y','comdCart_y','curCart_z','comdCart_z']\n",
    "\n",
    "# Model parameters\n",
    "MODEL_TYPE = \"ffnn_regression\"\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "VALID_TRAIN_RATIO = 0.2\n",
    "MAX_TRIALS = 5 \n",
    "MAX_EXECUTIONS = 3\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_import(path):\n",
    "    # Read JSON file\n",
    "    with open(path,\"r\") as file :\n",
    "        data = json.load(file)\n",
    "    for variable in data :\n",
    "        globals()[f'{variable}'] = data[variable]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import():\n",
    "    # Load DataFrame\n",
    "    full_df=pd.read_feather(DIRNAME+\"data/feather/\"+DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)+\".feather\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    full_df=full_df.drop([\"t_Sec\",\"t_nSec\",\"Fx1\",\"Fy1\",\"Fz1\",\"Tx1\",\"Ty1\",\"Tz1\"],axis=1)\n",
    "\n",
    "    # Transform position from absolute to relative\n",
    "    for i in range(len(CARTESIAN_COLUMNS)):\n",
    "        full_df[CARTESIAN_COLUMNS[i]] = full_df[CARTESIAN_COLUMNS[i]] + OFFSET_XYZ[i//2]\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classes(df):\n",
    "        for i in range(0,df.shape[0]):\n",
    "            # Recovering cartesian positions\n",
    "            x = df['curCart_x'][i]\n",
    "            y = df['curCart_y'][i]\n",
    "            # Calculating classes\n",
    "            f1 = y - 2*x\n",
    "            f2 = y + 2*x\n",
    "            f3 = y - x/2\n",
    "            f4 = y + x/2\n",
    "            # class 1\n",
    "            if (f1<0) & (f3>=0):\n",
    "                df.loc[i,'position']='class 1'\n",
    "            # class 2\n",
    "            elif (f3<0) & (f4>=0):\n",
    "                df.loc[i,'position']='class 2'\n",
    "            # class 3\n",
    "            elif (f4<0) & (f2>=0):\n",
    "                df.loc[i,'position']='class 3'\n",
    "            # class 4\n",
    "            elif (f2<0) & (f1<=0):\n",
    "                df.loc[i,'position']='class 4'\n",
    "            # class 5\n",
    "            elif (f1>0) & (f3<=0):\n",
    "                df.loc[i,'position']='class 5'\n",
    "            # class 6\n",
    "            elif (f3>0) & (f4<=0):\n",
    "                df.loc[i,'position']='class 6'\n",
    "            # class 7\n",
    "            elif (f4>0) & (f2<=0):\n",
    "                df.loc[i,'position']='class 7'\n",
    "            # class 8\n",
    "            elif (f2>0) & (f1>=0):\n",
    "                df.loc[i,'position']='class 8'\n",
    "        return df\n",
    "\n",
    "def time_series_transform(df):\n",
    "    timeseries_df = timeseries_dataset_from_array(df, None, sequence_length=10,shuffle=False)\n",
    "    el = []\n",
    "    for element in timeseries_df:\n",
    "        el.append(element.numpy())\n",
    "    el_full = np.concatenate(el, axis=0)\n",
    "    return el_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    shuffled_df=df.sample(frac=1,random_state=1)\n",
    "\n",
    "    # X & Y datasets\n",
    "\n",
    "    if MODEL_TYPE == \"ffnn_regression\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Regression Y\n",
    "        Y = shuffled_df[['curCart_x','curCart_y','curCart_z']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"ffnn_classification\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Classification Y\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        Y_unbinarized = shuffled_df[['position']].to_numpy()\n",
    "\n",
    "        # Classes to matrix using Binarizer\n",
    "        binarizer = LabelBinarizer().fit(Y_unbinarized)\n",
    "        Y = binarizer.transform(Y_unbinarized)\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_regression\" :\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['curCart_x','curCart_y']][9:].to_numpy())\n",
    "        Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_classification\" :\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key','position']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE11):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['position']][9:].to_numpy())\n",
    "        Y_unbinarized = np.concatenate(Y, axis=0)\n",
    "\n",
    "        # Classes to matrix using Binarizer\n",
    "        binarizer = LabelBinarizer().fit(Y_unbinarized)\n",
    "        Y = binarizer.transform(Y_unbinarized)\n",
    "\n",
    "\n",
    "\n",
    "    else :\n",
    "        raise ValueError(\"MODEL_TYPE parameter is not valid\")\n",
    "\n",
    "\n",
    "    # Train / Test / Validation dataset\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=TEST_TRAIN_RATIO, random_state=2)\n",
    "    X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_Train, Y_Train, test_size=VALID_TRAIN_RATIO, random_state=3)\n",
    "\n",
    "    return X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - FFNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):    # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class FFNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):    # Main\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Tune number of layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                    )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):        # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class CNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layers\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"mse\", \n",
    "                    metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - FFNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                     )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                      loss=\"categorical_crossentropy\", \n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layer\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"categorical_crossentropy\", \n",
    "                    metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 - RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_RandomSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.RandomSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=MAX_EXECUTIONS,\n",
    "        overwrite=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_GridSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.GridSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        overwrite=True,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3 - Bayesian Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_BayesianSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        alpha=0.0001,\n",
    "        beta=2.6,\n",
    "        overwrite=True,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2665201316.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    path =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "path ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON directory scan\n",
    "CONFIG_DIR=DIRNAME+\"config\"\n",
    "JSON_files = [file for file in os.listdir(CONFIG_DIR) if file.endswith(\".json\")]\n",
    "JSON_files.sort()\n",
    "\n",
    "counter = 3\n",
    "\n",
    "for JSON_file in JSON_files :\n",
    "    # Import\n",
    "    json_import(CONFIG_DIR+\"/\"+JSON_file)\n",
    "    print(\">> File Imported\")\n",
    "    # Prepare\n",
    "    data_df = data_import()\n",
    "    # Split\n",
    "    X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid=data_split(data_df)\n",
    "    print(\">> Dataset Split\")\n",
    "\n",
    "    # Model selector\n",
    "    if (MODEL_TYPE == \"ffnn_regression\") :\n",
    "        model = FFNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"ffnn_classification\") :\n",
    "        model = FFNN_Classification_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_regression\") :\n",
    "        model = CNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_classification\") :\n",
    "        model = CNN_Classification_Model()\n",
    "\n",
    "    print(\">> Model Loaded\")\n",
    "\n",
    "    tuners_list = [KT_RandomSearch(model),KT_GridSearch(model),KT_BayesianSearch(model)]\n",
    "\n",
    "    print(Y_Test[0])\n",
    "\n",
    "    for i in range(len(tuners_list)) :\n",
    "        tuner = tuners_list[i]\n",
    "        print(\">> Tuner Searching\")\n",
    "        # Search\n",
    "        tuner.search(X_Train, Y_Train, epochs=15, validation_data=(X_Valid, Y_Valid),callbacks=[keras.callbacks.TensorBoard('results/tensorboard/')])\n",
    "        # Export results\n",
    "        best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "\n",
    "        # Save Results\n",
    "        with open(DIRNAME+\"results/models/\"+DATA_FOLDER+\"_\"+MODEL_TYPE+\"_\"+str(VERSION_NB)+\"_\"+str(counter)+\".obj\",\"w\") as result_file :\n",
    "            json.dump(best_hps.values,result_file)\n",
    "\n",
    "        counter+=1\n",
    "\n",
    "    print(\"Program Successful !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 15:59:30.900894: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11215/11215 [==============================] - 81s 7ms/step - loss: 18.8332 - mae: 2.6848 - rmse: 2.9383 - rsquared: 0.8720 - val_loss: 1.8046 - val_mae: 0.9029 - val_rmse: 1.0097 - val_rsquared: 0.9877\n",
      "Epoch 2/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 2.4116 - mae: 0.9169 - rmse: 1.0332 - rsquared: 0.9835 - val_loss: 1.1687 - val_mae: 0.6371 - val_rmse: 0.7150 - val_rsquared: 0.9920\n",
      "Epoch 3/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 1.6667 - mae: 0.6867 - rmse: 0.7777 - rsquared: 0.9886 - val_loss: 0.6084 - val_mae: 0.4447 - val_rmse: 0.5084 - val_rsquared: 0.9959\n",
      "Epoch 4/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 1.3506 - mae: 0.5908 - rmse: 0.6698 - rsquared: 0.9907 - val_loss: 10.9531 - val_mae: 2.1264 - val_rmse: 2.3937 - val_rsquared: 0.9253\n",
      "Epoch 5/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 1.1613 - mae: 0.5303 - rmse: 0.6014 - rsquared: 0.9920 - val_loss: 0.6112 - val_mae: 0.4024 - val_rmse: 0.4578 - val_rsquared: 0.9958\n",
      "Epoch 6/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.9869 - mae: 0.4855 - rmse: 0.5510 - rsquared: 0.9932 - val_loss: 0.3073 - val_mae: 0.2947 - val_rmse: 0.3316 - val_rsquared: 0.9979\n",
      "Epoch 7/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.9098 - mae: 0.4519 - rmse: 0.5127 - rsquared: 0.9937 - val_loss: 0.4356 - val_mae: 0.3450 - val_rmse: 0.3914 - val_rsquared: 0.9970\n",
      "Epoch 8/40\n",
      "11215/11215 [==============================] - 72s 6ms/step - loss: 0.8329 - mae: 0.4318 - rmse: 0.4904 - rsquared: 0.9943 - val_loss: 20.1361 - val_mae: 3.0242 - val_rmse: 3.4859 - val_rsquared: 0.8628\n",
      "Epoch 9/40\n",
      "11215/11215 [==============================] - 79s 7ms/step - loss: 0.7825 - mae: 0.4102 - rmse: 0.4663 - rsquared: 0.9946 - val_loss: 0.4228 - val_mae: 0.3318 - val_rmse: 0.3784 - val_rsquared: 0.9971\n",
      "Epoch 10/40\n",
      "11215/11215 [==============================] - 80s 7ms/step - loss: 0.7098 - mae: 0.3898 - rmse: 0.4429 - rsquared: 0.9951 - val_loss: 6.1668 - val_mae: 1.3955 - val_rmse: 1.5835 - val_rsquared: 0.9575\n",
      "Epoch 11/40\n",
      "11215/11215 [==============================] - 74s 7ms/step - loss: 0.7056 - mae: 0.3864 - rmse: 0.4391 - rsquared: 0.9951 - val_loss: 0.3594 - val_mae: 0.2769 - val_rmse: 0.3128 - val_rsquared: 0.9975\n",
      "Epoch 12/40\n",
      "11215/11215 [==============================] - 71s 6ms/step - loss: 0.6912 - mae: 0.3753 - rmse: 0.4264 - rsquared: 0.9953 - val_loss: 0.3092 - val_mae: 0.2738 - val_rmse: 0.3117 - val_rsquared: 0.9979\n",
      "Epoch 13/40\n",
      "11215/11215 [==============================] - 76s 7ms/step - loss: 0.6835 - mae: 0.3765 - rmse: 0.4281 - rsquared: 0.9953 - val_loss: 4.0929 - val_mae: 0.9743 - val_rmse: 1.1283 - val_rsquared: 0.9721\n",
      "Epoch 14/40\n",
      "11215/11215 [==============================] - 75s 7ms/step - loss: 0.6413 - mae: 0.3636 - rmse: 0.4136 - rsquared: 0.9956 - val_loss: 0.2615 - val_mae: 0.2539 - val_rmse: 0.2848 - val_rsquared: 0.9982\n",
      "Epoch 15/40\n",
      "11215/11215 [==============================] - 79s 7ms/step - loss: 0.6121 - mae: 0.3601 - rmse: 0.4093 - rsquared: 0.9958 - val_loss: 0.6567 - val_mae: 0.3647 - val_rmse: 0.4169 - val_rsquared: 0.9955\n",
      "Epoch 16/40\n",
      "11215/11215 [==============================] - 79s 7ms/step - loss: 0.5994 - mae: 0.3517 - rmse: 0.3996 - rsquared: 0.9959 - val_loss: 0.3872 - val_mae: 0.3059 - val_rmse: 0.3459 - val_rsquared: 0.9973\n",
      "Epoch 17/40\n",
      "11215/11215 [==============================] - 81s 7ms/step - loss: 0.5956 - mae: 0.3513 - rmse: 0.3987 - rsquared: 0.9959 - val_loss: 0.6141 - val_mae: 0.3704 - val_rmse: 0.4237 - val_rsquared: 0.9958\n",
      "Epoch 18/40\n",
      "11215/11215 [==============================] - 89s 8ms/step - loss: 0.6400 - mae: 0.3566 - rmse: 0.4052 - rsquared: 0.9956 - val_loss: 0.8817 - val_mae: 0.4200 - val_rmse: 0.4747 - val_rsquared: 0.9940\n",
      "Epoch 19/40\n",
      "11215/11215 [==============================] - 89s 8ms/step - loss: 0.5524 - mae: 0.3410 - rmse: 0.3870 - rsquared: 0.9962 - val_loss: 0.2941 - val_mae: 0.2680 - val_rmse: 0.3041 - val_rsquared: 0.9980\n",
      "Epoch 20/40\n",
      "11215/11215 [==============================] - 88s 8ms/step - loss: 0.6152 - mae: 0.3545 - rmse: 0.4026 - rsquared: 0.9958 - val_loss: 0.9114 - val_mae: 0.4596 - val_rmse: 0.5203 - val_rsquared: 0.9937\n",
      "Epoch 21/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.5484 - mae: 0.3350 - rmse: 0.3806 - rsquared: 0.9962 - val_loss: 0.5231 - val_mae: 0.3471 - val_rmse: 0.3927 - val_rsquared: 0.9964\n",
      "Epoch 22/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.6016 - mae: 0.3454 - rmse: 0.3923 - rsquared: 0.9959 - val_loss: 0.7634 - val_mae: 0.3981 - val_rmse: 0.4503 - val_rsquared: 0.9948\n",
      "Epoch 23/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.5202 - mae: 0.3305 - rmse: 0.3754 - rsquared: 0.9964 - val_loss: 1.0865 - val_mae: 0.4868 - val_rmse: 0.5485 - val_rsquared: 0.9925\n",
      "Epoch 24/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.4770 - mae: 0.3155 - rmse: 0.3584 - rsquared: 0.9967 - val_loss: 0.1978 - val_mae: 0.2309 - val_rmse: 0.2618 - val_rsquared: 0.9986\n",
      "Epoch 25/40\n",
      "11215/11215 [==============================] - 74s 7ms/step - loss: 0.5212 - mae: 0.3298 - rmse: 0.3747 - rsquared: 0.9964 - val_loss: 0.2619 - val_mae: 0.2508 - val_rmse: 0.2838 - val_rsquared: 0.9982\n",
      "Epoch 26/40\n",
      "11215/11215 [==============================] - 73s 7ms/step - loss: 0.5590 - mae: 0.3406 - rmse: 0.3871 - rsquared: 0.9962 - val_loss: 0.2484 - val_mae: 0.2376 - val_rmse: 0.2686 - val_rsquared: 0.9983\n",
      "Epoch 27/40\n",
      "11215/11215 [==============================] - 76s 7ms/step - loss: 0.5399 - mae: 0.3363 - rmse: 0.3822 - rsquared: 0.9963 - val_loss: 1.1826 - val_mae: 0.5255 - val_rmse: 0.5972 - val_rsquared: 0.9919\n",
      "Epoch 28/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.5438 - mae: 0.3368 - rmse: 0.3827 - rsquared: 0.9963 - val_loss: 0.2434 - val_mae: 0.2444 - val_rmse: 0.2771 - val_rsquared: 0.9983\n",
      "Epoch 29/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.5406 - mae: 0.3326 - rmse: 0.3780 - rsquared: 0.9963 - val_loss: 0.6917 - val_mae: 0.4145 - val_rmse: 0.4710 - val_rsquared: 0.9953\n",
      "Epoch 30/40\n",
      "11215/11215 [==============================] - 79s 7ms/step - loss: 0.4821 - mae: 0.3171 - rmse: 0.3602 - rsquared: 0.9967 - val_loss: 0.3966 - val_mae: 0.2987 - val_rmse: 0.3377 - val_rsquared: 0.9973\n",
      "Epoch 31/40\n",
      "11215/11215 [==============================] - 75s 7ms/step - loss: 0.5235 - mae: 0.3271 - rmse: 0.3709 - rsquared: 0.9964 - val_loss: 0.2332 - val_mae: 0.2401 - val_rmse: 0.2717 - val_rsquared: 0.9984\n",
      "Epoch 32/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.5329 - mae: 0.3331 - rmse: 0.3785 - rsquared: 0.9963 - val_loss: 0.2354 - val_mae: 0.2434 - val_rmse: 0.2751 - val_rsquared: 0.9984\n",
      "Epoch 33/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.5337 - mae: 0.3376 - rmse: 0.3837 - rsquared: 0.9963 - val_loss: 0.7193 - val_mae: 0.4042 - val_rmse: 0.4636 - val_rsquared: 0.9951\n",
      "Epoch 34/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.5021 - mae: 0.3263 - rmse: 0.3708 - rsquared: 0.9966 - val_loss: 0.8373 - val_mae: 0.4444 - val_rmse: 0.5080 - val_rsquared: 0.9943\n",
      "Epoch 35/40\n",
      "11215/11215 [==============================] - 79s 7ms/step - loss: 0.5789 - mae: 0.3448 - rmse: 0.3918 - rsquared: 0.9960 - val_loss: 0.2746 - val_mae: 0.2656 - val_rmse: 0.3004 - val_rsquared: 0.9981\n",
      "Epoch 36/40\n",
      "11215/11215 [==============================] - 81s 7ms/step - loss: 0.5402 - mae: 0.3361 - rmse: 0.3820 - rsquared: 0.9963 - val_loss: 0.6868 - val_mae: 0.3748 - val_rmse: 0.4226 - val_rsquared: 0.9953\n",
      "Epoch 37/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.5598 - mae: 0.3419 - rmse: 0.3883 - rsquared: 0.9961 - val_loss: 0.3869 - val_mae: 0.3122 - val_rmse: 0.3534 - val_rsquared: 0.9974\n",
      "Epoch 38/40\n",
      "11215/11215 [==============================] - 80s 7ms/step - loss: 0.5422 - mae: 0.3357 - rmse: 0.3814 - rsquared: 0.9963 - val_loss: 0.2588 - val_mae: 0.2528 - val_rmse: 0.2880 - val_rsquared: 0.9982\n",
      "Epoch 39/40\n",
      "11215/11215 [==============================] - 78s 7ms/step - loss: 0.5245 - mae: 0.3340 - rmse: 0.3795 - rsquared: 0.9964 - val_loss: 0.3127 - val_mae: 0.2726 - val_rmse: 0.3102 - val_rsquared: 0.9979\n",
      "Epoch 40/40\n",
      "11215/11215 [==============================] - 77s 7ms/step - loss: 0.5421 - mae: 0.3400 - rmse: 0.3864 - rsquared: 0.9963 - val_loss: 0.3798 - val_mae: 0.2941 - val_rmse: 0.3329 - val_rsquared: 0.9974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f12c6f450f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_cnn_regression(hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layers\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"mse\", \n",
    "                    metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "path = \"/home/quentin/Documents/ai_force_torque/results/models/versuch_f1_cnn_regression_1_3.obj\"\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "with open(path,\"r\") as file :\n",
    "    hp.values = json.load(file)\n",
    "\n",
    "json_import(DIRNAME+\"config/test1_2.json\")\n",
    "df = data_import()\n",
    "X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid=data_split(df)\n",
    "\n",
    "final_model = build_cnn_regression(hp)\n",
    "final_model.fit(X_Train,Y_Train,epochs=40,verbose=1,validation_data=[X_Valid,Y_Valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
