{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file regroups every kind of test for model training : \n",
    "- The first file & library imports are mandatory\n",
    "- The last exports are mandatory\n",
    "- The model training depends on the choice of algorithm\n",
    "\n",
    "There is an option to automate the training using togglable parameters \n",
    "Make sure to have set up the correct file structure using env_setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 04:18:42.761078: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 04:18:42.761119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 04:18:42.762414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-04 04:18:42.769531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-04 04:18:43.571387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# TensorFlow / Keras Part\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Input, GlobalAveragePooling1D, Dropout\n",
    "from keras.preprocessing import timeseries_dataset_from_array\n",
    "import keras_tuner as kt\n",
    "\n",
    "# SKLearn Part\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic constant file parameters\n",
    "FILENAME=\"model_training.ipynb\"\n",
    "DIRNAME=os.path.abspath(FILENAME).replace(FILENAME,'')\n",
    "\n",
    "# Datafile parameters\n",
    "DATA_FOLDER = \"versuch_f1\"\n",
    "VERSION_NB = 1\n",
    "DF_POINTS_RANGE = 11\n",
    "DF_POINTS_LENGTH = 3000\n",
    "\n",
    "# Data parameters\n",
    "OFFSET_XYZ=[-578.6,261.4,-375]\n",
    "CARTESIAN_COLUMNS=['curCart_x','comdCart_x','curCart_y','comdCart_y','curCart_z','comdCart_z']\n",
    "\n",
    "# Model parameters\n",
    "MODEL_TYPE = \"ffnn_regression\"\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "VALID_TRAIN_RATIO = 0.2\n",
    "MAX_TRIALS = 5 \n",
    "MAX_EXECUTIONS = 3\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_import(path):\n",
    "    # Read JSON file\n",
    "    with open(path,\"r\") as file :\n",
    "        data = json.load(file)\n",
    "    for variable in data :\n",
    "        globals()[f'{variable}'] = data[variable]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import():\n",
    "    # Load DataFrame\n",
    "    full_df=pd.read_feather(DIRNAME+\"data/feather/\"+DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)+\".feather\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    full_df=full_df.drop([\"t_Sec\",\"t_nSec\",\"Fx1\",\"Fy1\",\"Fz1\",\"Tx1\",\"Ty1\",\"Tz1\"],axis=1)\n",
    "\n",
    "    # Transform position from absolute to relative\n",
    "    for i in range(len(CARTESIAN_COLUMNS)):\n",
    "        full_df[CARTESIAN_COLUMNS[i]] = full_df[CARTESIAN_COLUMNS[i]] + OFFSET_XYZ[i//2]\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classes(df):\n",
    "        for i in range(0,df.shape[0]):\n",
    "            # Recovering cartesian positions\n",
    "            x = df['curCart_x'][i]\n",
    "            y = df['curCart_y'][i]\n",
    "            # Calculating classes\n",
    "            f1 = y - 2*x\n",
    "            f2 = y + 2*x\n",
    "            f3 = y - x/2\n",
    "            f4 = y + x/2\n",
    "            # class 1\n",
    "            if (f1<0) & (f3>=0):\n",
    "                df.loc[i,'position']='class 1'\n",
    "            # class 2\n",
    "            elif (f3<0) & (f4>=0):\n",
    "                df.loc[i,'position']='class 2'\n",
    "            # class 3\n",
    "            elif (f4<0) & (f2>=0):\n",
    "                df.loc[i,'position']='class 3'\n",
    "            # class 4\n",
    "            elif (f2<0) & (f1<=0):\n",
    "                df.loc[i,'position']='class 4'\n",
    "            # class 5\n",
    "            elif (f1>0) & (f3<=0):\n",
    "                df.loc[i,'position']='class 5'\n",
    "            # class 6\n",
    "            elif (f3>0) & (f4<=0):\n",
    "                df.loc[i,'position']='class 6'\n",
    "            # class 7\n",
    "            elif (f4>0) & (f2<=0):\n",
    "                df.loc[i,'position']='class 7'\n",
    "            # class 8\n",
    "            elif (f2>0) & (f1>=0):\n",
    "                df.loc[i,'position']='class 8'\n",
    "        return df\n",
    "\n",
    "def time_series_transform(df):\n",
    "    timeseries_df = timeseries_dataset_from_array(df, None, sequence_length=10,shuffle=False)\n",
    "    el = []\n",
    "    for element in timeseries_df:\n",
    "        el.append(element.numpy())\n",
    "    el_full = np.concatenate(el, axis=0)\n",
    "    return el_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    shuffled_df=df.sample(frac=1,random_state=1)\n",
    "\n",
    "    # X & Y datasets\n",
    "\n",
    "    if MODEL_TYPE == \"ffnn_regression\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Regression Y\n",
    "        Y = shuffled_df[['curCart_x','curCart_y','curCart_z']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"ffnn_classification\" :\n",
    "        X = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                    'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                    'Fx','Fy','Fz','Tx','Ty','Tz']].to_numpy()\n",
    "        # Classification Y\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        Y = shuffled_df[['position']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_regression\" :\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['curCart_x','curCart_y']][9:].to_numpy())\n",
    "        Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    elif MODEL_TYPE == \"cnn_classification\" :\n",
    "        shuffled_df=add_classes(shuffled_df)\n",
    "        select_df = shuffled_df[['exT_A1','exT_A2','exT_A3','exT_A4','exT_A5','exT_A6','exT_A7',\n",
    "                                 'msT_A1','msT_A2','msT_A3','msT_A4','msT_A5','msT_A6','msT_A7',\n",
    "                                 'Fx','Fy','Fz','Tx','Ty','Tz','curCart_x','curCart_y','curCart_z','key','position']]\n",
    "        # Split the dataset into smaller datasets based on key\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'key{i}_{j}'] = select_df.loc[select_df['key'] == '{0},{1}'.format(i,j)]\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'key{i}_{j}_{k}'] = time_series_transform(locals()[f'key{i}_{j}'].iloc[:,k])\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                locals()[f'ts_key{i}_{j}'] = []\n",
    "                for k in range(0,20):\n",
    "                    locals()[f'ts_key{i}_{j}'].append(locals()[f'key{i}_{j}_{k}'])\n",
    "                locals()[f'ts_key{i}_{j}'] = np.stack(locals()[f'ts_key{i}_{j}'], axis=-1)\n",
    "        # Define Input\n",
    "        X = []\n",
    "        for i in range(0,DF_POINTS_RANGE):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                X.append(locals()[f'ts_key{i}_{j}'])\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        # Define Output\n",
    "        Y = []\n",
    "        for i in range(0,DF_POINTS_RANGE11):\n",
    "            for j in range(0,DF_POINTS_RANGE):\n",
    "                Y.append(locals()[f'key{i}_{j}'][['position']][9:].to_numpy())\n",
    "        Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    else :\n",
    "        raise ValueError(\"MODEL_TYPE parameter is not valid\")\n",
    "\n",
    "\n",
    "    # Train / Test / Validation dataset\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=TEST_TRAIN_RATIO, random_state=2)\n",
    "    X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_Train, Y_Train, test_size=VALID_TRAIN_RATIO, random_state=3)\n",
    "\n",
    "    return X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - FFNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):    # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class FFNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):    # Main\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Tune number of layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                    )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):        # Private\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "def rsquared(y_true,y_pred):    # Private\n",
    "    RSS =  backend.sum(backend.square( y_true- y_pred ))\n",
    "    TSS = backend.sum(backend.square( y_true - backend.mean(y_true)))\n",
    "    return 1-(RSS/TSS)\n",
    "\n",
    "class CNN_Regression_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"mae\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layers\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"mse\", \n",
    "                    metrics=[\"mae\",rmse,rsquared])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - FFNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Dense(20, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "            model.add(Dense(\n",
    "                    # Tune number of units\n",
    "                    units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    # Tune activation function\n",
    "                    activation = hp.Choice(\"activation\", [\"relu\",\"tanh\"])\n",
    "                            )\n",
    "                     )\n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.2))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                      loss=\"categorical_crossentropy\", \n",
    "                      metrics=[\"accuracy\",\"precision\",\"recall\",\"f1_score\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classification_Model(kt.HyperModel):\n",
    "\n",
    "    # Search metrics\n",
    "    objective=\"accuracy\"\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input convolutional Layer\n",
    "        model.add(Conv1D(\n",
    "                    # input shape\n",
    "                    input_shape = (10,20),\n",
    "                    # filters\n",
    "                    filters = hp.Int('conv_input_filter', min_value=16, max_value=128, step=16),\n",
    "                    # kernel_size\n",
    "                    kernel_size = hp.Choice('conv_input_kernel', values=[3,5]),\n",
    "                    # activation\n",
    "                    activation = hp.Choice('conv_input_activation', ['relu','tanh']),\n",
    "                    padding=\"same\"\n",
    "                ))\n",
    "        \n",
    "        # Hidden convolutional Layer\n",
    "        for i in range (hp.Int(\"num_layers\",1,6)):\n",
    "            model.add(Conv1D(\n",
    "                        # filters\n",
    "                        filters = hp.Int(f'conv_{i}_filter', min_value=16, max_value=128, step=16),\n",
    "                        # kernel_size\n",
    "                        kernel_size = hp.Choice(f'conv_{i}_kernel', values=[3,5]),\n",
    "                        # activation\n",
    "                        activation = hp.Choice(f'conv_{i}_activation', ['relu','tanh']),\n",
    "                        padding=\"same\"\n",
    "                    ))\n",
    "        \n",
    "        # Tune whether to use dropout\n",
    "        if hp.Boolean(\"dropout\"):\n",
    "            model.add(Dropout(rate=0.25))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        \n",
    "        # Define the optimizer learning rate as hyperparameter\n",
    "        lr = hp.Float(\"lr\", min_value=1e-3, max_value=1e-2, sampling=\"log\")\n",
    "        # Compile model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=\"categorical_crossentropy\", \n",
    "                    metrics=[\"accuracy\",\"precision\",\"recall\",\"f1_score\"])\n",
    "        return model\n",
    "\n",
    "    # Custom fit function\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\", [16, 32, 64]),**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 - RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_RandomSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.RandomSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=MAX_EXECUTIONS,\n",
    "        overwrite=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_GridSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.GridSearch(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        overwrite=True,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3 - Bayesian Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KT_BayesianSearch(Selected_Model):\n",
    "    # Keras search function\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        Selected_Model,\n",
    "        objective=Selected_Model.objective,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        alpha=0.0001,\n",
    "        beta=2.6,\n",
    "        overwrite=True,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        directory=DIRNAME+\"/results/search/\"+MODEL_TYPE,\n",
    "        project_name=DATA_FOLDER+\"_\"+str(DF_POINTS_LENGTH)+\"_\"+str(VERSION_NB)\n",
    "    )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test1_0.json']\n",
      ">> File Imported\n",
      ">> Dataset Split\n",
      ">> Model Loaded\n",
      "Reloading Tuner from /home/quentin/Documents/ai_force_torque//results/search/ffnn_regression/versuch_f1_3000_1/tuner0.json\n",
      "Reloading Tuner from /home/quentin/Documents/ai_force_torque//results/search/ffnn_regression/versuch_f1_3000_1/tuner0.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error reloading `Oracle` from existing project. If you did not mean to reload from an existing project, change the `project_name` or pass `overwrite=True` when creating the `Tuner`. Found existing project at: /home/quentin/Documents/ai_force_torque//results/search/ffnn_regression/versuch_f1_3000_1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:757\u001b[0m, in \u001b[0;36mOracle.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mreload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_oracle_fname())\n\u001b[1;32m    758\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/stateful.py:71\u001b[0m, in \u001b[0;36mStateful.reload\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reloads this object using `set_state`.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    fname: A string, the file name to restore from.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_state(utils\u001b[39m.\u001b[39;49mload_json(fname))\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/tuners/bayesian.py:225\u001b[0m, in \u001b[0;36mBayesianOptimizationOracle.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mset_state(state)\n\u001b[0;32m--> 225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_initial_points \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39;49m\u001b[39mnum_initial_points\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_initial_points'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/quentin/Documents/ai_force_torque/model_training.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model \u001b[39m=\u001b[39m CNN_Classification_Model()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m>> Model Loaded\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m tuners_list \u001b[39m=\u001b[39m [KT_GridSearch(model),KT_BayesianSearch(model)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#,KT_RandomSearch(model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tuners_list)) :\n",
      "\u001b[1;32m/home/quentin/Documents/ai_force_torque/model_training.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mKT_BayesianSearch\u001b[39m(Selected_Model):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Keras search function\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tuner \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39;49mBayesianOptimization(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         Selected_Model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         objective\u001b[39m=\u001b[39;49mSelected_Model\u001b[39m.\u001b[39;49mobjective,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         max_trials\u001b[39m=\u001b[39;49mMAX_TRIALS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         alpha\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m2.6\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         tune_new_entries\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         allow_new_entries\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         directory\u001b[39m=\u001b[39;49mDIRNAME\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/results/search/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mMODEL_TYPE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         project_name\u001b[39m=\u001b[39;49mDATA_FOLDER\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(DF_POINTS_LENGTH)\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(VERSION_NB)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quentin/Documents/ai_force_torque/model_training.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tuner\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/tuners/bayesian.py:394\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    367\u001b[0m     hypermodel\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    380\u001b[0m ):\n\u001b[1;32m    381\u001b[0m     oracle \u001b[39m=\u001b[39m BayesianOptimizationOracle(\n\u001b[1;32m    382\u001b[0m         objective\u001b[39m=\u001b[39mobjective,\n\u001b[1;32m    383\u001b[0m         max_trials\u001b[39m=\u001b[39mmax_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         max_consecutive_failed_trials\u001b[39m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[0;32m--> 394\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(oracle\u001b[39m=\u001b[39;49moracle, hypermodel\u001b[39m=\u001b[39;49mhypermodel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:122\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m hypermodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mrun_trial \u001b[39mis\u001b[39;00m Tuner\u001b[39m.\u001b[39mrun_trial:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`hypermodel` if the user defines the search space in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing a `HyperModel` instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    123\u001b[0m     oracle\u001b[39m=\u001b[39;49moracle,\n\u001b[1;32m    124\u001b[0m     hypermodel\u001b[39m=\u001b[39;49mhypermodel,\n\u001b[1;32m    125\u001b[0m     directory\u001b[39m=\u001b[39;49mdirectory,\n\u001b[1;32m    126\u001b[0m     project_name\u001b[39m=\u001b[39;49mproject_name,\n\u001b[1;32m    127\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m    128\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_model_size \u001b[39m=\u001b[39m max_model_size\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:129\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mand\u001b[39;00m backend\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tuner_fname()):\n\u001b[1;32m    128\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReloading Tuner from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tuner_fname()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_populate_initial_space()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:459\u001b[0m, in \u001b[0;36mBaseTuner.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reloads this object from its project directory.\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_worker():\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moracle\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    460\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tuner_fname())\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:759\u001b[0m, in \u001b[0;36mOracle.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_oracle_fname())\n\u001b[1;32m    758\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    760\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError reloading `Oracle` from existing project. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    761\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you did not mean to reload from an existing project, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchange the `project_name` or pass `overwrite=True` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwhen creating the `Tuner`. Found existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproject at: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_project_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m# Empty the ongoing_trials and send them for retry.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39mfor\u001b[39;00m _, trial \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mongoing_trials\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error reloading `Oracle` from existing project. If you did not mean to reload from an existing project, change the `project_name` or pass `overwrite=True` when creating the `Tuner`. Found existing project at: /home/quentin/Documents/ai_force_torque//results/search/ffnn_regression/versuch_f1_3000_1"
     ]
    }
   ],
   "source": [
    "# JSON directory scan\n",
    "CONFIG_DIR=DIRNAME+\"config\"\n",
    "JSON_files = [file for file in os.listdir(CONFIG_DIR) if file.endswith(\".json\")]\n",
    "JSON_files.sort()\n",
    "\n",
    "print(JSON_files)\n",
    "\n",
    "for JSON_file in JSON_files :\n",
    "    # Import\n",
    "    json_import(CONFIG_DIR+\"/\"+JSON_file)\n",
    "    print(\">> File Imported\")\n",
    "    # Prepare\n",
    "    data_df = data_import()\n",
    "    # Split\n",
    "    X_Train,X_Test,X_Valid,Y_Train,Y_Test,Y_Valid=data_split(data_df)\n",
    "    print(\">> Dataset Split\")\n",
    "\n",
    "    # Model selector\n",
    "    if (MODEL_TYPE == \"ffnn_regression\") :\n",
    "        model = FFNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"ffnn_classification\") :\n",
    "        model = FFNN_Classification_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_regression\") :\n",
    "        model = CNN_Regression_Model()\n",
    "    elif (MODEL_TYPE == \"cnn_classification\") :\n",
    "        model = CNN_Classification_Model()\n",
    "\n",
    "    print(\">> Model Loaded\")\n",
    "\n",
    "    tuners_list = [KT_GridSearch(model),KT_BayesianSearch(model)]\n",
    "    #,KT_RandomSearch(model)\n",
    "\n",
    "    for i in range(len(tuners_list)) :\n",
    "        tuner = tuners_list[i]\n",
    "        print(\">> Tuner Searching\")\n",
    "        # Search\n",
    "        tuner.search(X_Train, Y_Train, epochs=15, validation_data=(X_Valid, Y_Valid),callbacks=[keras.callbacks.TensorBoard('results/tensorboard/')])\n",
    "        # Export results\n",
    "        best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "        # Save Results\n",
    "        with open(DIRNAME+\"results/models/\"+DATA_FOLDER+\"_\"+MODEL_TYPE+\"_\"+str(VERSION_NB)+\"_\"+str(i+1)+\".obj\",\"w\") as result_file :\n",
    "            json.dump(best_hps.values,result_file)\n",
    "\n",
    "    print(\"Program Successful !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
